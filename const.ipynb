{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "provincial-champagne",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appointed-digest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-coffee",
   "metadata": {},
   "source": [
    "## 1. Dictionary entries\n",
    "### 1.1. Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define target_names\n",
    "target_names_regr = ['t2m']\n",
    "target_names_classi = ['hw_bin_15SD', 'hw_bin_1SD']\n",
    "## Define lead times (in units of the user-defined timestep)\n",
    "lead_times = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "## Define different balancing possibilities ('undersampling', 'oversampling', 'none')\n",
    "balance_types = ['undersampling', 'oversampling']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-thinking",
   "metadata": {},
   "source": [
    "### 1.2. Data & time periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select dataset ('combi', 'ERA5', 'ERA20C')\n",
    "### combi available [1981, 2018]\n",
    "### ERA5 available [1950, 2020]\n",
    "### ERA20C available [1900, 2009]\n",
    "dataset = 'combi'\n",
    "\n",
    "## Specify train_full, train, vali & test time period splits for each dataset\n",
    "splits_combi = np.array([[1981, 2009], [1981, 2000], [2001, 2009], [2010, 2018]]).T\n",
    "splits_ERA5 = np.array([[1950, 2000], [1950, 1990], [1991, 2000], [2001, 2020]]).T\n",
    "splits_ERA20C = np.array([[1900, 1989], [1900, 1974], [1975, 1989], [1990, 2009]]).T\n",
    "\n",
    "## Cross-validation ('none', 'nested')\n",
    "### 'none' has no loops but uses the train_full-test-train-vali partitions manually specified above instead\n",
    "### 'nested' has an inner (train-validation) and an outer loop (train_full-test)\n",
    "cv_type = 'none'\n",
    "### Folds in outer and inner loop\n",
    "if cv_type == 'none': num_outer_folds = None; num_inner_folds = None\n",
    "elif cv_type == 'nested': num_outer_folds = 5; num_inner_folds = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "postal-sweet",
   "metadata": {},
   "source": [
    "### 1.3. Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select temporal resolution (in days e.g., '5D' or months e.g., '1M')\n",
    "timestep = '7D'\n",
    "\n",
    "## Simple or rolling mean? ('simple', 'rolling')\n",
    "mean_type = 'rolling'\n",
    "\n",
    "## Define number of lags you want to have in the predictors e.g. for num_lags = 3, (lead_time, lead_time + 2 weeks before the \n",
    "## prediction date) will be taken into account\n",
    "num_lags = 4\n",
    "## Add lower resolution (x 2,3,4) features for soil moisture and SST predictors\n",
    "all_lags_and_means = False\n",
    "\n",
    "## Select season\n",
    "initial_month = 5\n",
    "final_month = 9\n",
    "\n",
    "## Percentile for extreme day definition (only releavant for the 2in7 index)\n",
    "percentile = 0.95\n",
    "\n",
    "## Minimum consecutive extreme days to be considered a heat wave (only relevant for the 2in7 index)\n",
    "min_duration = 3\n",
    "\n",
    "## Geopotential level in hPa\n",
    "geopotential_level = '500.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-lightning",
   "metadata": {},
   "source": [
    "### 1.4. Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop predictors? If True: the 11 predictors out of 18 that show lower lagged correlation (selection has been done manually) are dropped\n",
    "### Rem: regularized linear models benefit from being fed with all 18 predictors, while RFs work better when given only the 7 most relevant ones\n",
    "### Feed them with a different dataset?\n",
    "drop_predictors = True\n",
    "\n",
    "## Feature selection? If True: feature selection with Random Forest to keep features whose importances sum 0.9\n",
    "feature_selection = False\n",
    "min_sum_importances = 0.9\n",
    "\n",
    "## Dimensionality reduction? If True: Principal Component Analysis (PCA)\n",
    "pca = False\n",
    "min_cum_exp_var = 0.9\n",
    "\n",
    "## Do you want to run preprocessing 2 again or use saved values?\n",
    "prepro2 = True\n",
    "## Do you want to train the ML models again (True) or read the ML model's forecasts from memory (False)?\n",
    "train_models = True\n",
    "\n",
    "## Do you want to optimize hyperparameters (True) or use the last saved set of hyperparameters to train the ML model (False)? \n",
    "optimize_rf_hyperparam = False\n",
    "## Search type ('rdm', 'exhaustive')\n",
    "### 'exhaustive' explores all possible hyperparameter combinations in the grid\n",
    "### 'rdm' explores a only a subset of the possible hyperparameter combinations\n",
    "hp_search_type = 'exhaustive'\n",
    "### Specify number of hyperparameter combinations to try out (only valid for rdm search)\n",
    "num_hp_set_candidates = 10\n",
    "\n",
    "## Apply regularization?\n",
    "### Regression\n",
    "reg_regr = False\n",
    "### Classification\n",
    "# True per default\n",
    "\n",
    "## Metric to optimize \n",
    "### Regression ('RMSE', 'Corr', 'RMSE_Corr')\n",
    "metric_regr = 'RMSE'\n",
    "### Classification\n",
    "metric_classi = 'ROC AUC'\n",
    "\n",
    "## Do you want to explore overfitting? Plots the metric on the validation set against the model complexity\n",
    "explore_overfitting = False\n",
    "\n",
    "## Do you want to print a Decission Tree?\n",
    "show_tree = False\n",
    "\n",
    "## Should the figures show and be saved with a title?\n",
    "plot_fig_title = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-yugoslavia",
   "metadata": {},
   "source": [
    "### 1.5. Verbosity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-quest",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose verbosity level\n",
    "###        0 : Nothing printed nor plotted\n",
    "###        1 : Section & subsection titles (highlighted) printed, tables and plots shown\n",
    "###        2 : Add subtitles (****), coefficients, importances and best hyperparameters\n",
    "###        3 : Add comments\n",
    "###        4 : Add hyperparameter optimization steps & print of lists and numbers \n",
    "###        5 : Add dataset prints & scatter plots & EOF plots\n",
    "verbosity = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-academy",
   "metadata": {},
   "source": [
    "### 1.6. Machinery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run on... ('CFC', 'Euler')\n",
    "server = 'CFC'\n",
    "\n",
    "## Number of cores\n",
    "if server == 'CFC': n_cores = 30\n",
    "elif server == 'Euler': n_cores = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-transsexual",
   "metadata": {},
   "source": [
    "### 1.7. Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "if server == 'CFC':\n",
    "    ## Paths to data sources\n",
    "    ### Reanalysis\n",
    "    path_eraint = '/s2s/shared_data/Datasets/ERA-Int/'\n",
    "    path_era5_land = '/s2s/weiriche/heat_waves/reanalysis/ERA5-Land/'\n",
    "    path_hadisst = '/s2s/shared_data/Datasets/HadISST/'\n",
    "    path_bernatj_data = '/home/bernatj/data/'\n",
    "    path_eobs = '/s2s/shared_data/Datasets/E-OBS/'\n",
    "    ### Forecasts\n",
    "    path_ecmwf = '/s2s/weiriche/heat_waves/s2s_forecasts/ECMWF_2020_runs/'\n",
    "\n",
    "    ## Paths to datasets with predictors\n",
    "    path_input_orig_pred = '/s2s/weiriche/heat_waves/inputs/predictors/original/'\n",
    "    path_input_prepro_pred = '/s2s/weiriche/heat_waves/inputs/predictors/preprocessed/'\n",
    "\n",
    "    ## Path to target heat wave data\n",
    "    path_input_targ = '/s2s/weiriche/heat_waves/inputs/targets/'\n",
    "\n",
    "    ## Path to metrics\n",
    "    path_metrics = '/s2s/weiriche/heat_waves/metrics/'\n",
    "\n",
    "    ## Path to results\n",
    "    path_hyperparam = '/s2s/weiriche/heat_waves/results/hyperparameters/'\n",
    "    path_time_series = '/s2s/weiriche/heat_waves/results/time_series/'\n",
    "\n",
    "    ## Path to plots\n",
    "    path_plots_prediction = '/home/weiriche/heat_waves/plots/Prediction/'\n",
    "\n",
    "elif server == 'Euler':\n",
    "    # Path to CFC on Euler\n",
    "    path_cfc_on_euler = '/cluster/home/weiriche/heat_waves_cfc/' \n",
    "    \n",
    "    ## Paths to data sources\n",
    "    ### Reanalysis (none, since Euler will not be used for the preprocessing, but for running the prediction algorithms)\n",
    "    path_eraint = None\n",
    "    path_era5_land = None\n",
    "    path_hadisst = None\n",
    "    path_bernatj_data = None\n",
    "    path_eobs = None\n",
    "    ### Forecasts\n",
    "    path_ecmwf = path_cfc_on_euler + 's2s_forecasts/ECMWF_2020_runs/'\n",
    "\n",
    "    ## Paths to datasets with predictors\n",
    "    path_input_orig_pred = None\n",
    "    path_input_prepro_pred = path_cfc_on_euler + 'inputs/predictors/preprocessed/'\n",
    "\n",
    "    ## Path to target heat wave data\n",
    "    path_input_targ = path_cfc_on_euler + 'inputs/targets/'\n",
    "\n",
    "    ## Path to metrics\n",
    "    path_metrics = path_cfc_on_euler + 'metrics/'\n",
    "\n",
    "    ## Path to results\n",
    "    path_hyperparam = path_cfc_on_euler + 'results/hyperparameters/'\n",
    "    path_time_series = path_cfc_on_euler + 'results/time_series/'\n",
    "\n",
    "    ## Path to plots\n",
    "    path_plots_prediction = path_cfc_on_euler + 'plots/Prediction/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-satisfaction",
   "metadata": {},
   "source": [
    "### 1.8. Grid point boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Central Europe (CE) [45°N,55°N], [5°,15°E]\n",
    "ce = np.array([[55.,45.], [5.,15.]]).T\n",
    "ce_obs = np.array([[44.875,54.875], [4.875,14.875]]).T\n",
    "\n",
    "# Central Europe Large (CE_large) [30°N,70°N], [50°W,60°E]\n",
    "ce_large_left = np.array([[70.,30.], [310.,357.5]]).T\n",
    "ce_large_right = np.array([[70.,30.], [0.,60.]]).T\n",
    "ce_obs_large = np.array([[29.875,69.875], [-50.125,59.875]]).T\n",
    "\n",
    "# Mediterranean Sea (MED) [30°N,45°N], [0°,25°E]\n",
    "med = np.array([[45.,30.], [0.,25.]]).T\n",
    "## North western box (NWMED) [35°N,45°N], [0°,15°E]\n",
    "nwmed = np.array([[45.,35.], [0.,15.]]).T\n",
    "## South eastern box (SEMED) [30°N,40°N], [15°,35°E]\n",
    "semed = np.array([[40.,30.], [15.,35.]]).T\n",
    "\n",
    "# Northern Atlantic (NA)\n",
    "## Baltic [50°N,65°N], [0°W,25°W]\n",
    "baltic = np.array([[65.5,50.5], [-24.5, 0.5]]).T\n",
    "## Cold blob box (CBNA) [45°N,60°N], [15°W,40°W]\n",
    "cbna = np.array([[60.5,45.5], [-40., -15.]]).T\n",
    "## North western box (NWNA) [42.5°N,52.5°N], [52.5°W,40°W]\n",
    "nwna = np.array([[52.5,42.5], [-52.5, -40.]]).T\n",
    "## South eastern box (SENA) [35°N,42.5°N], [35°W,20°W]\n",
    "sena = np.array([[42.5,35.], [-35., -20.]]).T\n",
    "\n",
    "# Pacific\n",
    "## El Niño Southern Oscillation (ENSO) [5°S,5°N], [120°W, 170°W]\n",
    "enso = np.array([[5.,-5.], [-170., -120.]]).T\n",
    "## Pacific Region [10°N, 20°N],[ 180°, 110°W]\n",
    "pac = np.array([[20.,10.], [180., 250.]]).T\n",
    "## Caribbean Region [10°N, 25°N] [85°W, 65°W]\n",
    "carib = np.array([[25.,10.], [275., 295.]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-outside",
   "metadata": {},
   "source": [
    "### 1.9. Long predictor names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "normal-nepal",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_predictor_names = {'t2m_x': 'Temperature',\n",
    "                            'z': 'Geopotential height',\n",
    "                            'rain': 'Precipitation', \n",
    "                            'sm': 'Soil moisture', \n",
    "                            'sm_deeper': 'Deeper soil moisture', \n",
    "                            'sm_deepest': 'Deepest soil moisture', \n",
    "                            'snao': 'Summer North Atlantic Oscillation', \n",
    "                            'sea': 'Summer East Atlantic pattern',\n",
    "                            'sst_med': 'Mediterranean SST',\n",
    "                            'sst_nwmed': 'North-western Mediterranean SST', \n",
    "                            'sst_semed': 'South-eastern Mediterranean SST', \n",
    "                            'sst_baltic': 'Baltic SST', \n",
    "                            'sst_cbna': 'Cold blob North Atlantic SST', \n",
    "                            'sst_nwna': 'North-western North Atlantic SST', \n",
    "                            'sst_sena': 'South-eastern North Atlantic SST',\n",
    "                            'na_index': 'North Atlantic index',\n",
    "                            'sst_enso': 'El Ninyo Southern Oscillation', \n",
    "                            'pcd_index': 'Pacific Caribbean Dipole index'\n",
    "                        }\n",
    "units = {'t2m': '^0C',\n",
    "         't2m_x': '^0C',\n",
    "         'z': 'm^2/s^2',\n",
    "         'rain': 'mm', \n",
    "         'sm': 'm^3/m^3', \n",
    "         'sm_deeper': 'm^3/m^3', \n",
    "         'sm_deepest': 'm^3/m^3', \n",
    "         'snao': 'm^2/s^2', \n",
    "         'sea': 'm^2/s^2',\n",
    "         'sst_med': '^0C',\n",
    "         'sst_nwmed': '^0C', \n",
    "         'sst_semed': '^0C', \n",
    "         'sst_baltic': '^0C', \n",
    "         'sst_cbna': '^0C', \n",
    "         'sst_nwna': '^0C', \n",
    "         'sst_sena': '^0C',\n",
    "         'na_index': '^0C',\n",
    "         'sst_enso': '^0C', \n",
    "         'pcd_index': 'm'\n",
    "                        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-genius",
   "metadata": {},
   "source": [
    " **------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-remove",
   "metadata": {},
   "source": [
    " **------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-wales",
   "metadata": {},
   "source": [
    "## 2. Save definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advised-calendar",
   "metadata": {},
   "source": [
    "### 2.1. Structre train-vali-test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-irrigation",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33851/1820236926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build boxes into 3D xarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m splits = xr.concat(\n\u001b[0m\u001b[1;32m      3\u001b[0m     [xr.DataArray(\n\u001b[1;32m      4\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'edge'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'slice_type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "# Build slices into 3D xarray\n",
    "splits = xr.concat(\n",
    "    [xr.DataArray(\n",
    "            X,\n",
    "            dims = ['edge', 'slice_type'],\n",
    "            coords = {'slice_type': ['train_full', 'train', 'vali', 'test'], 'edge': ['start', 'end']})\n",
    "            for X in (splits_combi, splits_ERA5, splits_ERA20C)],\n",
    "            dim = 'dataset').assign_coords(dataset = ['combi', 'ERA5',  'ERA20C'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-falls",
   "metadata": {},
   "source": [
    "### 2.2. Structure boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-consensus",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_33851/2122969614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Build boxes into 3D xarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m boxes = xr.concat(\n\u001b[0m\u001b[1;32m      3\u001b[0m     [xr.DataArray(\n\u001b[1;32m      4\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'edge'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'axis'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xr' is not defined"
     ]
    }
   ],
   "source": [
    "# Build boxes into 3D xarray\n",
    "boxes = xr.concat(\n",
    "    [xr.DataArray(\n",
    "            X,\n",
    "            dims = ['edge','axis'],\n",
    "            coords = {'axis': ['latitude', 'longitude'], 'edge': ['start', 'end']})\n",
    "            for X in (ce, ce_obs, ce_large_left, ce_large_right, ce_obs_large, med, nwmed, semed, baltic, cbna, nwna, sena, enso, pac, carib)],\n",
    "            dim = 'box').assign_coords(box = ['ce', 'ce_obs',  'ce_large_left', 'ce_large_right', 'ce_obs_large', 'med', 'nwmed', 'semed', \n",
    "                                              'baltic', 'cbna', 'nwna', 'sena', 'enso', 'pac', 'carib'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-weapon",
   "metadata": {},
   "source": [
    "### 2.3. Adapt paths depending on time period and dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-juvenile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute overall start and end years\n",
    "start_year = int(splits.sel(dataset = dataset, slice_type = 'train_full', edge = 'start'))\n",
    "end_year = int(splits.sel(dataset = dataset, slice_type = 'test', edge = 'end'))\n",
    "\n",
    "# Adapt paths\n",
    "path_input_prepro_pred = path_input_prepro_pred + dataset + '_dataset/'\n",
    "path_input_targ = path_input_targ + dataset + '_dataset/'\n",
    "path_metrics = path_metrics + str(start_year) + '-' + str(end_year) + '_period_' + dataset + '_dataset/'\n",
    "path_hyperparam = path_hyperparam + str(start_year) + '-' + str(end_year) + '_period_' + dataset + '_dataset/'\n",
    "path_time_series = path_time_series + str(start_year) + '-' + str(end_year) + '_period_' + dataset + '_dataset/'\n",
    "path_plots_prediction = path_plots_prediction + str(start_year) + '-' + str(end_year) + '_period_' + dataset + '_dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exciting-nashville",
   "metadata": {},
   "source": [
    "### 2.4. Make dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-truth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dictionary\n",
    "dictionary = { \n",
    "  'target_names': target_names_regr + target_names_classi,\n",
    "  'target_names_regr': target_names_regr,\n",
    "  'target_names_classi': target_names_classi,\n",
    "  'lead_times': lead_times,\n",
    "  'balance_types':balance_types,\n",
    "  'dataset': dataset,\n",
    "  'cv_type': cv_type,\n",
    "  'hp_search_type': hp_search_type,\n",
    "  'num_hp_set_candidates': num_hp_set_candidates,\n",
    "  'num_outer_folds': num_outer_folds,\n",
    "  'num_inner_folds': num_inner_folds,\n",
    "  'timestep': timestep,\n",
    "  'timestep_num': int(timestep[:-1]),\n",
    "  'num_lags': num_lags,\n",
    "  'all_lags_and_means': all_lags_and_means,\n",
    "  'initial_month': initial_month,\n",
    "  'final_month': final_month,\n",
    "  'splits': splits,\n",
    "  'start_year': start_year,\n",
    "  'end_year': end_year,\n",
    "  'boxes': boxes,\n",
    "  'mean_type': mean_type,\n",
    "  'percentile': percentile,\n",
    "  'min_duration': min_duration,\n",
    "  'geopotential_level': geopotential_level,\n",
    "  'drop_predictors': drop_predictors,\n",
    "  'feature_selection': feature_selection,\n",
    "  'min_sum_importances': min_sum_importances,\n",
    "  'pca': pca,\n",
    "  'min_cum_exp_var': min_cum_exp_var,\n",
    "  'prepro2': prepro2,\n",
    "  'train_models': train_models,\n",
    "  'optimize_rf_hyperparam': optimize_rf_hyperparam,\n",
    "  'reg_regr': reg_regr,\n",
    "#  'reg_classi': reg_classi,\n",
    "  'metric_regr': metric_regr,\n",
    "  'metric_classi': metric_classi,\n",
    "  'explore_overfitting': explore_overfitting,\n",
    "  'show_tree': show_tree,\n",
    "  'plot_fig_title': plot_fig_title,\n",
    "  'n_cores': n_cores,\n",
    "  'path_eraint': path_eraint, \n",
    "  'path_era5_land': path_era5_land,\n",
    "  'path_hadisst': path_hadisst,\n",
    "  'path_bernatj_data': path_bernatj_data,\n",
    "  'path_eobs': path_eobs,\n",
    "  'path_ecmwf': path_ecmwf,\n",
    "  'path_input_orig_pred': path_input_orig_pred,\n",
    "  'path_input_prepro_pred': path_input_prepro_pred,\n",
    "  'path_input_targ': path_input_targ,\n",
    "  'path_metrics': path_metrics,\n",
    "  'path_hyperparam': path_hyperparam,\n",
    "  'path_time_series': path_time_series,\n",
    "  'path_plots_prediction': path_plots_prediction,\n",
    "  'verbosity': verbosity,\n",
    "  'long_predictor_names': long_predictor_names,  \n",
    "  'units': units,\n",
    "  'server': server\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-enzyme",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-iac_env]",
   "language": "python",
   "name": "conda-env-.conda-iac_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
